"""
Cloud Functions ì§„ì…ì : ì¹´ì¹´ì˜¤ ì›¹íˆ° ì£¼ê°„ ì°¨íŠ¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

ì´ í•¨ìˆ˜ëŠ” HTTP íŠ¸ë¦¬ê±°ë¡œ ì‹¤í–‰ë˜ë©°, ì „ì²´ ELT íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
- Extract: ì¹´ì¹´ì˜¤ ì›¹íˆ° APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
- Load Raw: GCSì— JSON ì›ë³¸ ì €ì¥
- Transform: ë°ì´í„° íŒŒì‹± ë° ì •ê·œí™”
- Load Refined: BigQueryì— ì •ì œëœ ë°ì´í„° ì €ì¥
"""

import json
import logging
import os
from datetime import date
from typing import Optional

import functions_framework

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
import sys
from pathlib import Path

# Cloud Functionsì—ì„œëŠ” /workspaceê°€ ë£¨íŠ¸
# ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
if os.path.exists('/workspace'):
    project_root = Path('/workspace')
    sys.path.insert(0, str(project_root))
else:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©: functions/pipeline_functionì—ì„œ srcë¡œ ì ‘ê·¼
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    # src ë””ë ‰í† ë¦¬ë„ ê²½ë¡œì— ì¶”ê°€
    src_path = project_root / 'src'
    if src_path.exists():
        sys.path.insert(0, str(src_path))

from src.extract import extract_webtoon_chart, try_api_endpoints, SORT_OPTIONS
from src.parse import parse_html_file
from src.parse_api import parse_api_response
from src.transform import transform_and_save
from src.utils import setup_logging, get_chart_jsonl_path, get_dim_webtoon_jsonl_path

# ë¡œê¹… ì„¤ì • (ë¨¼ì € ì„¤ì •)
setup_logging()
logger = logging.getLogger(__name__)

# GCS/BigQuery ì—…ë¡œë“œëŠ” ì„ íƒì ìœ¼ë¡œ import (ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ ì—†ì„ ìˆ˜ ìˆìŒ)
try:
    from src.upload_gcs import upload_chart_data_to_gcs
    UPLOAD_GCS_AVAILABLE = True
except ImportError:
    UPLOAD_GCS_AVAILABLE = False
    logger.warning("GCS ì—…ë¡œë“œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")

try:
    from src.upload_bigquery import (
        upload_dim_webtoon,
        upload_fact_weekly_chart,
        get_bigquery_client,
    )
    UPLOAD_BIGQUERY_AVAILABLE = True
except ImportError:
    UPLOAD_BIGQUERY_AVAILABLE = False
    logger.warning("BigQuery ì—…ë¡œë“œ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")

# í™˜ê²½ ë³€ìˆ˜
GCS_BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'kakao-webtoon-raw')
BIGQUERY_PROJECT_ID = os.getenv('BIGQUERY_PROJECT_ID', 'kakao-webtoon-collector')
BIGQUERY_DATASET_ID = os.getenv('BIGQUERY_DATASET_ID', 'kakao_webtoon')

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

# GCS/BigQuery ì—…ë¡œë“œ ëª¨ë“ˆ import ì „ì— logger ì„¤ì • í•„ìš”
# (ìœ„ì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)


@functions_framework.http
def main(request):
    """
    Cloud Functions HTTP íŠ¸ë¦¬ê±° ì§„ì…ì 
    
    Args:
        request: Flask Request ê°ì²´
    
    Returns:
        HTTP ì‘ë‹µ (JSON)
    """
    try:
        # ìš”ì²­ ë³¸ë¬¸ íŒŒì‹±
        request_json = request.get_json(silent=True)
        if request_json is None:
            request_json = {}
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        chart_date_str = request_json.get('date')
        if chart_date_str:
            try:
                chart_date = date.fromisoformat(chart_date_str)
            except ValueError:
                logger.error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {chart_date_str}")
                return {'error': f'Invalid date format: {chart_date_str}'}, 400
        else:
            chart_date = date.today()
        
        sort_keys = request_json.get('sort_keys', ['popularity'])  # ê¸°ë³¸ê°’: ì „ì²´ ì¸ê¸°ìˆœ
        collect_all_weekdays = request_json.get('collect_all_weekdays', False)
        limit = request_json.get('limit')  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
        
        # ì‹¤í–‰ ë‚ ì§œì˜ ìš”ì¼ ê³„ì‚° (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
        weekday_index = chart_date.weekday()  # 0=Monday, 6=Sunday
        weekday_map = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']
        current_weekday = weekday_map[weekday_index]
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: date={chart_date}, weekday={current_weekday}, sort_keys={sort_keys}, collect_all_weekdays={collect_all_weekdays}")
        
        all_success = True
        
        # APIë¥¼ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì—¬ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
        logger.info("API í˜¸ì¶œí•˜ì—¬ ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘...")
        logger.info(f"âš ï¸  ì£¼ì˜: ì¹´ì¹´ì˜¤ ì›¹íˆ° APIëŠ” ê³¼ê±° ë‚ ì§œì˜ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                   f"ìš”ì²­í•œ ë‚ ì§œ({chart_date})ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ í˜„ì¬ ì‹œì ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        
        # collect_all_weekdaysê°€ Trueì´ë©´ ëª¨ë“  ìš”ì¼ ìˆ˜ì§‘, Falseì´ë©´ í˜„ì¬ ìš”ì¼ë§Œ ìˆ˜ì§‘
        if collect_all_weekdays:
            api_data = try_api_endpoints(
                weekday=None,  # ëª¨ë“  ìš”ì¼ ìˆ˜ì§‘ ëª¨ë“œ
                filter_type='ì „ì²´',  # ì „ì²´ í•„í„°
                collect_all_weekdays=True,
                sort_key=None,  # ì •ë ¬ì€ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œ ì²˜ë¦¬
                chart_date=chart_date  # ë©”íƒ€ë°ì´í„°ìš© (API í˜¸ì¶œì—ëŠ” ì˜í–¥ ì—†ìŒ)
            )
        else:
            # í˜„ì¬ ìš”ì¼ë§Œ ìˆ˜ì§‘ (ë§¤ì¼ ìˆ˜ì§‘ ëª¨ë“œ)
            api_data = try_api_endpoints(
                weekday=current_weekday,  # í˜„ì¬ ìš”ì¼ë§Œ ìˆ˜ì§‘
                filter_type='ì „ì²´',  # ì „ì²´ í•„í„°
                collect_all_weekdays=False,
                sort_key=None,  # ì •ë ¬ì€ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œ ì²˜ë¦¬
                chart_date=chart_date  # ë©”íƒ€ë°ì´í„°ìš© (API í˜¸ì¶œì—ëŠ” ì˜í–¥ ì—†ìŒ)
            )
        
        if api_data is None:
            logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return {'error': 'Failed to collect data'}, 500
        
        # Step 1: Load Raw (GCSì— JSON ì›ë³¸ ì €ì¥)
        if UPLOAD_GCS_AVAILABLE:
            logger.info("GCSì— ì›ë³¸ ë°ì´í„° ì €ì¥ ì¤‘...")
            from tempfile import NamedTemporaryFile
            
            with NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as tmp_file:
                json.dump(api_data, tmp_file, ensure_ascii=False, indent=2)
                tmp_path = Path(tmp_file.name)
            
            try:
                # ê¸°ë³¸ JSON ì €ì¥ (sort_key ì—†ì´)
                gcs_success = upload_chart_data_to_gcs(
                    chart_date,
                    sort_key=None,
                    json_file_path=tmp_path,
                    dry_run=False
                )
                if not gcs_success:
                    logger.warning("GCS ì—…ë¡œë“œ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if tmp_path.exists():
                    tmp_path.unlink()
        else:
            logger.info("GCS ì—…ë¡œë“œ ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # Step 2 & 3: Parse & Transform & Load Refined (ê° ì •ë ¬ ì˜µì…˜ë³„ë¡œ ì²˜ë¦¬)
        # ì£¼ì˜: parse_api_responseëŠ” ê° sort_keyë³„ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš© (Cloud Functionsì˜ /tmp ì‚¬ìš©)
        import tempfile
        temp_dir = Path(tempfile.gettempdir()) / 'kakao_webtoon_pipeline'
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ)
        os.environ['DATA_DIR'] = str(temp_dir)
        
        # ê° ì •ë ¬ ì˜µì…˜ë³„ë¡œ ì²˜ë¦¬
        for sort_key in sort_keys:
            if sort_key not in SORT_OPTIONS:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì •ë ¬ í‚¤: {sort_key}, ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            sort_name = SORT_OPTIONS[sort_key]
            logger.info(f"\n{'='*60}")
            logger.info(f"ì •ë ¬ ì˜µì…˜: {sort_name} ({sort_key})")
            logger.info(f"{'='*60}")
            
            try:
                # API ë°ì´í„°ì— ì •ë ¬ í‚¤ ì¶”ê°€
                api_data_with_sort = api_data.copy()
                api_data_with_sort['_sort_key'] = sort_key
                
                # ì •ë ¬ëœ ë°ì´í„° íŒŒì‹± (parse_api_responseê°€ sort_keyë¥¼ ë°›ì•„ì„œ ì •ë ¬í•¨)
                sorted_parsed_data = parse_api_response(api_data_with_sort, sort_key=sort_key)
                
                if len(sorted_parsed_data) == 0:
                    logger.warning(f"ì •ë ¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ ({sort_name})")
                    continue
                
                # ì •ë ¬ ì •ë³´ ì¶”ê°€
                for item in sorted_parsed_data:
                    item['_sort_key'] = sort_key
                    item['_sort_name'] = sort_name
                
                # ë°ì´í„° ë³€í™˜ ë° ì €ì¥
                logger.info(f"ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì‹œì‘ ({sort_name})...")
                success = transform_and_save(sorted_parsed_data, chart_date, sort_key)
                
                if success:
                    # ì €ì¥ëœ JSONL íŒŒì¼ì„ BigQueryì— ì—…ë¡œë“œ
                    if UPLOAD_BIGQUERY_AVAILABLE:
                        from src.utils import get_dim_webtoon_jsonl_path, get_chart_jsonl_path
                        
                        # dim_webtoon ì—…ë¡œë“œ (ì²« ë²ˆì§¸ ì •ë ¬ ì˜µì…˜ì¼ ë•Œë§Œ)
                        if sort_key == sort_keys[0]:
                            dim_jsonl_path = get_dim_webtoon_jsonl_path()
                            if dim_jsonl_path.exists():
                                logger.info(f"dim_webtoon.jsonl íŒŒì¼ ë°œê²¬, BigQuery ì—…ë¡œë“œ ì‹œì‘: {dim_jsonl_path}")
                                try:
                                    upload_success = upload_dim_webtoon(jsonl_path=dim_jsonl_path, dry_run=False)
                                    if upload_success:
                                        logger.info("âœ… dim_webtoon BigQuery ì—…ë¡œë“œ ì„±ê³µ")
                                    else:
                                        logger.error("dim_webtoon BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                                except Exception as e:
                                    logger.error(f"dim_webtoon BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                    import traceback
                                    traceback.print_exc()
                        
                        # fact_weekly_chart ì—…ë¡œë“œ
                        fact_jsonl_path = get_chart_jsonl_path(chart_date, sort_key)
                        if fact_jsonl_path.exists():
                            logger.info(f"fact_weekly_chart.jsonl íŒŒì¼ ë°œê²¬, BigQuery ì—…ë¡œë“œ ì‹œì‘: {fact_jsonl_path}")
                            try:
                                upload_success = upload_fact_weekly_chart(
                                    chart_date=chart_date,
                                    sort_key=sort_key,
                                    jsonl_path=fact_jsonl_path,
                                    dry_run=False
                                )
                                if upload_success:
                                    logger.info(f"âœ… fact_weekly_chart BigQuery ì—…ë¡œë“œ ì„±ê³µ ({sort_name})")
                                else:
                                    logger.error(f"fact_weekly_chart BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨ ({sort_name})")
                            except Exception as e:
                                logger.error(f"fact_weekly_chart BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({sort_name}): {e}")
                                import traceback
                                traceback.print_exc()
                        else:
                            logger.warning(f"fact_weekly_chart.jsonl íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {fact_jsonl_path}")
                    else:
                        logger.info("BigQuery ì—…ë¡œë“œ ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    
                    # ì •ë ¬ë³„ GCS ì—…ë¡œë“œ
                    if UPLOAD_GCS_AVAILABLE:
                        logger.info(f"ì •ë ¬ë³„ GCS ì—…ë¡œë“œ ì‹œì‘ ({sort_name})...")
                        # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ì— ì €ì¥
                        from tempfile import NamedTemporaryFile
                        with NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as tmp_file:
                            # ì •ë ¬ëœ API ë°ì´í„° ì¬êµ¬ì„±
                            sorted_api_data = api_data.copy()
                            sorted_api_data['_sort_key'] = sort_key
                            json.dump(sorted_api_data, tmp_file, ensure_ascii=False, indent=2)
                            tmp_path = Path(tmp_file.name)
                        
                        try:
                            gcs_success = upload_chart_data_to_gcs(
                                chart_date,
                                sort_key=sort_key,
                                json_file_path=tmp_path,
                                dry_run=False
                            )
                            if gcs_success:
                                logger.info(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ ({sort_name})")
                            else:
                                logger.warning(f"GCS ì—…ë¡œë“œ ì‹¤íŒ¨ ({sort_name}), ê³„ì† ì§„í–‰...")
                        finally:
                            if tmp_path.exists():
                                tmp_path.unlink()
                    
                    logger.info(f"âœ… ì •ë ¬ ì˜µì…˜ '{sort_name}' ìˆ˜ì§‘ ì™„ë£Œ!")
                else:
                    logger.error(f"ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì‹¤íŒ¨ ({sort_name})")
                    all_success = False
                    continue
                    
            except Exception as e:
                logger.error(f"ì •ë ¬ ì˜µì…˜ '{sort_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                all_success = False
        
        if all_success:
            logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            return {'status': 'success', 'date': str(chart_date)}, 200
        else:
            logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ")
            return {'status': 'partial_failure', 'date': str(chart_date)}, 500
            
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}, 500

